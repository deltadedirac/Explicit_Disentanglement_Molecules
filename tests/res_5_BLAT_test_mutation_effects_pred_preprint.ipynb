{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68209549",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at '<a href=\"#papermill-error-cell\">In [4]</a>'.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "539bbed6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-02-22T13:16:41.155482Z",
     "iopub.status.busy": "2024-02-22T13:16:41.155164Z",
     "iopub.status.idle": "2024-02-22T13:16:41.166303Z",
     "shell.execute_reply": "2024-02-22T13:16:41.165731Z"
    },
    "executionInfo": {
     "elapsed": 31456,
     "status": "ok",
     "timestamp": 1708322038670,
     "user": {
      "displayName": "variational alignment",
      "userId": "06771419765440515286"
     },
     "user_tz": -60
    },
    "id": "pxUnsooJ-z7m",
    "outputId": "38a079a0-e76a-4c87-97c3-1f46b8d3b283",
    "papermill": {
     "duration": 0.022918,
     "end_time": "2024-02-22T13:16:41.168609",
     "exception": false,
     "start_time": "2024-02-22T13:16:41.145691",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom google.colab import drive\\ndrive.mount('/content/gdrive/')\\n\\n#!pip install -q condacolab\\n#import condacolab\\n#condacolab.install()\\n\\n%cd /content/gdrive/MyDrive/Explicit_Disentanglement_Molecules/tests\\n!pip install gpytorch mlflow tensorboard tensorboardx torch-tb-profiler pyfiglet dpath logomaker biopython pandas numpy tqdm confuse seaborn nbconvert\\n!pip install papermill[all]\\n!pip install torchvision\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive/')\n",
    "\n",
    "#!pip install -q condacolab\n",
    "#import condacolab\n",
    "#condacolab.install()\n",
    "\n",
    "%cd /content/gdrive/MyDrive/Explicit_Disentanglement_Molecules/tests\n",
    "!pip install gpytorch mlflow tensorboard tensorboardx torch-tb-profiler pyfiglet dpath logomaker biopython pandas numpy tqdm confuse seaborn nbconvert\n",
    "!pip install papermill[all]\n",
    "!pip install torchvision\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0641ab09",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "execution": {
     "iopub.execute_input": "2024-02-22T13:16:41.183108Z",
     "iopub.status.busy": "2024-02-22T13:16:41.182803Z",
     "iopub.status.idle": "2024-02-22T13:16:49.073985Z",
     "shell.execute_reply": "2024-02-22T13:16:49.073129Z"
    },
    "executionInfo": {
     "elapsed": 7139,
     "status": "ok",
     "timestamp": 1708322045806,
     "user": {
      "displayName": "variational alignment",
      "userId": "06771419765440515286"
     },
     "user_tz": -60
    },
    "id": "nFP-W7aK-z7o",
    "outputId": "fdedd7c6-9310-4602-e1d6-a560a1de8718",
    "papermill": {
     "duration": 7.900204,
     "end_time": "2024-02-22T13:16:49.076227",
     "exception": false,
     "start_time": "2024-02-22T13:16:41.176023",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/z/home/sgal/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not import libcpab, error was\n",
      "No module named 'libcpab'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pdb\n",
    "import torch, os\n",
    "import argparse, datetime\n",
    "import gc,time\n",
    "from tqdm import tqdm\n",
    "import __init__\n",
    "\n",
    "\n",
    "\n",
    "#from src.gp_cpab.src.transformation.gp_cpab import gp_cpab\n",
    "#from src.gp_cpab.src.transformation.gp_cpab_tmp import gp_cpab\n",
    "from src.gp_cpab.src.transformation.configManager import configManager\n",
    "from src.gp_cpab.src.extra.experiment_utilities import *\n",
    "\n",
    "from src.models.trainer_tmp import vae_trainer\n",
    "from src.unsuper.unsuper.data.mnist_data_loader import mnist_data_loader\n",
    "from src.unsuper.unsuper.data.perception_data_loader import perception_data_loader\n",
    "from src.unsuper.unsuper.helper.utility import model_summary\n",
    "from src.models.encoder_decoder import get_encoder, get_decoder, get_list_encoders, get_list_decoders\n",
    "from src.models import get_model\n",
    "\n",
    "from src.seqsDataLoader import seqsReader, seqsDatasetLoader, Sequence_Data_Loader\n",
    "from src.dataLoaderDiffeo import *\n",
    "\n",
    "#from src.models.experimental.vitae_ci_gp_detached_deepseq import vitae_ci_gp_no_deepseq\n",
    "from src.models.experimental.PGM_LA_latent_alignment import PGM_LA_latent_alignment\n",
    "from src.models.experimental.deepsequence import DeepSequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78a6a40a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-22T13:16:49.092126Z",
     "iopub.status.busy": "2024-02-22T13:16:49.091631Z",
     "iopub.status.idle": "2024-02-22T13:16:49.107903Z",
     "shell.execute_reply": "2024-02-22T13:16:49.107168Z"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1708322045807,
     "user": {
      "displayName": "variational alignment",
      "userId": "06771419765440515286"
     },
     "user_tz": -60
    },
    "id": "RdgYKtfQ-z7p",
    "papermill": {
     "duration": 0.027001,
     "end_time": "2024-02-22T13:16:49.110637",
     "exception": false,
     "start_time": "2024-02-22T13:16:49.083636",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse; import sys; sys.argv=['']; del sys\n",
    "import math\n",
    "\n",
    "def define_prior_init(channels, space='log'):\n",
    "    prior_init = []\n",
    "    base_alpha = channels-2\n",
    "\n",
    "    if space=='log':\n",
    "        #prior_init = [math.log(1e-6)]*channels\n",
    "        prior_init = [math.log(1e-6), *([ math.log(1/base_alpha) ]*base_alpha)]\n",
    "    else:\n",
    "        # Current Approach\n",
    "        #prior_init = [0.0, *([1/base_alpha]*base_alpha)]\n",
    "        #prior_init = [0.0, 0.0, *([1/base_alpha]*base_alpha)]\n",
    "\n",
    "\n",
    "        # Approach A for avoiding flat landscape in the optimization - avoiding local optima\n",
    "        prior_init = [1.0, 0.0, *([0.0]*base_alpha) ]\n",
    "\n",
    "    return prior_init\n",
    "\n",
    "def argparser():\n",
    "    \"\"\" Argument parser for the main script \"\"\"\n",
    "    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "    # Model settings\n",
    "    ms = parser.add_argument_group('Model settings')\n",
    "    ms.add_argument('--model', type=str, default='vitae_ci', help='model to train')\n",
    "    ms.add_argument('--ed_type', type=str, default='mlp,mlp', help='encoder/decoder type')\n",
    "    ms.add_argument('--stn_type', type=str, default='gp_cpab', help='transformation type to use')\n",
    "    ms.add_argument('--beta', type=float, default=16, help='beta value for beta-vae model') #16\n",
    "\n",
    "    # Training settings\n",
    "    ts = parser.add_argument_group('Training settings')\n",
    "    ts.add_argument('--n_epochs', type=int, default=500, help='number of epochs of training')\n",
    "    ts.add_argument('--eval_epoch', type=int, default=5, help='when to evaluate log(p(x))')\n",
    "    ts.add_argument('--batch_size', type=int, default=10, help='size of the batches') # batch=10,warmup=10, epochs=100\n",
    "    ts.add_argument('--warmup', type=int, default=5, help='number of warmup epochs for kl-terms')\n",
    "    ts.add_argument('--lr', type=float, default=1e-3, help='learning rate for adam optimizer') # 1e-5 for 2 sequence deepseq 2 seqs other# 1e-3, 1e-7 # weird case for 550 iters and 1e-4, works in trans but regular in reconstruction\n",
    "\n",
    "    # Paths to use\n",
    "    paths = parser.add_argument_group('Paths')\n",
    "    paths.add_argument('--path_orig', type=str, default=\"../data/set_preprint/BLAT500_gaps.fasta\", help='original sequence to deform')  #\"../data/WW10seeds_rawnoHoles.fasta\"  #orig_3aa.fasta orig_3aag.fasta\n",
    "    paths.add_argument('--path_aligned_orig', type=str, default=\"../data/set_preprint/BLAT500_alignment.fasta\", help='original sequence to deform')    #orig_3aa.fasta orig_3aag.fasta\n",
    "\n",
    "\n",
    "    paths.add_argument('--path_preexist_linear', type=str, default=\"../../models/CPABlinear3.pth\", help='prebuilt model using linear case')\n",
    "    paths.add_argument('--path_preexist_gp', type=str, default=\"../../models/CPABGPB3.pth\", help='prebuilt model using gp case')\n",
    "    paths.add_argument('--path_automated_report', type=str, default=\"../../Results\", help='path to save automatic report')\n",
    "    paths.add_argument('--logdir', type=str, default=\"../../Results\", help='where to store results')\n",
    "\n",
    "    gpsetup = parser.add_argument_group('GPSetup')\n",
    "    #gpsetup.add_argument('--Task', type=int, default = 8, help='Amount of channels in multitask-gp estimator')\n",
    "    #gpsetup.add_argument('--Initialization', type=list, default = [0.0, 0.143, 0.143, 0.143, 0.143, 0.143, 0.143, 0.143], help='multitaks-gp initialization')\n",
    "    gpsetup.add_argument('--Task', type=int, default = 22, help='Amount of channels in multitask-gp estimator')\n",
    "    gpsetup.add_argument('--Initialization', type=list, default = define_prior_init(22, space='default'), help='multitaks-gp initialization') # before it was 19 and works good in current setup with uniform distributed values in prior\n",
    "    #gpsetup.add_argument('--Task', type=int, default = 11, help='Amount of channels in multitask-gp estimator')\n",
    "    #gpsetup.add_argument('--Initialization', type=list, default = [0.0, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1], help='multitaks-gp initialization') #[0.0, 0.33, 0.33, 0.33] #[0.5, 0.5]\n",
    "    gpsetup.add_argument('--Lengthscale', type=float, default = 0.5, help='lengthscale on square-exponential kernel') # *** 0.5 *** #0.25, #0.5 + init gives better for 2 gaps case, # 1.25\n",
    "    gpsetup.add_argument('--noise_constraint', type=list, default = [1e-3,1e-1], help='noise constrain -- [lower_bound, upper_bound]') #16\n",
    "    gpsetup.add_argument('--Option', type=str, default = 'multitask', help='noise constrain') #16\n",
    "\n",
    "    # CPAB features\n",
    "    cpab = parser.add_argument_group('CPAB')\n",
    "    cpab.add_argument('--device', type=str, default=\"gpu\", help='device')\n",
    "    cpab.add_argument('--modeflag', type=str, default=\"1D\", help='dimensionality of tesselation')\n",
    "    cpab.add_argument('--window_grid', type=int, default=1750, help='number of tesselation cells') #6 80, the increasing of tess improve the performance when there are more samples to train the density estimator\n",
    "    cpab.add_argument('--channels', type=int, default=1750, help='amount of channels for estimation --deprecated')\n",
    "    cpab.add_argument('--interpolation_type', type=str, default=\"GP\", help='type of interpolation between maps')\n",
    "\n",
    "    # Hyper settings\n",
    "    hp = parser.add_argument_group('Variational settings')\n",
    "    hp.add_argument('--latent_dim', type=int, default=60, help='dimensionality of the latent space') #40, 5, 10, 40 is the bst one for WW\n",
    "    hp.add_argument('--density', type=str, default='softmax', help='output density')  # bernoulli  gaussian\n",
    "    hp.add_argument('--eq_samples', type=int, default=1, help='number of MC samples over the expectation over E_q(z|x)')\n",
    "    hp.add_argument('--iw_samples', type=int, default=1, help='number of importance weighted samples')\n",
    "\n",
    "\n",
    "    # Parse and return\n",
    "    args = parser.parse_args()\n",
    "    return args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b950e1f",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span id=\"papermill-error-cell\" style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">Execution using papermill encountered an exception here and stopped:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16f8c3c6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-02-22T13:16:49.132670Z",
     "iopub.status.busy": "2024-02-22T13:16:49.132427Z",
     "iopub.status.idle": "2024-02-22T13:16:49.441731Z",
     "shell.execute_reply": "2024-02-22T13:16:49.440621Z"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1708322045807,
     "user": {
      "displayName": "variational alignment",
      "userId": "06771419765440515286"
     },
     "user_tz": -60
    },
    "id": "k5DEpk2y-z7q",
    "outputId": "c490be20-dca0-42c9-ea70-961328f6dfbd",
    "papermill": {
     "duration": 0.321885,
     "end_time": "2024-02-22T13:16:49.443262",
     "exception": true,
     "start_time": "2024-02-22T13:16:49.121377",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'path_test'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m path \u001b[38;5;241m=\u001b[39m std\u001b[38;5;241m.\u001b[39mparserinfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath_orig\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m path_aligned \u001b[38;5;241m=\u001b[39m std\u001b[38;5;241m.\u001b[39mparserinfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath_aligned_orig\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m path_test \u001b[38;5;241m=\u001b[39m \u001b[43mstd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparserinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpath_test\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m path_preexist_model \u001b[38;5;241m=\u001b[39m std\u001b[38;5;241m.\u001b[39mparserinfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath_preexist_linear\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     18\u001b[0m path_preexist_modelGP \u001b[38;5;241m=\u001b[39m std\u001b[38;5;241m.\u001b[39mparserinfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath_preexist_gp\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/net/ged.nzcorp.net/z/home/sgal/ML_Projects/Explicit_Disentanglement_Molecules/src/gp_cpab/src/transformation/configManager.py:18\u001b[0m, in \u001b[0;36mconfigManager.parserinfo\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparserinfo\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/PGM_alignment/lib/python3.8/site-packages/dpath/util.py:14\u001b[0m, in \u001b[0;36mdeprecated.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     13\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/PGM_alignment/lib/python3.8/site-packages/dpath/util.py:36\u001b[0m, in \u001b[0;36mget\u001b[0;34m(obj, glob, separator, default)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;129m@deprecated\u001b[39m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(obj, glob, separator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m, default\u001b[38;5;241m=\u001b[39m_DEFAULT_SENTINEL):\n\u001b[0;32m---> 36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseparator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/PGM_alignment/lib/python3.8/site-packages/dpath/__init__.py:189\u001b[0m, in \u001b[0;36mget\u001b[0;34m(obj, glob, separator, default)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m default \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _DEFAULT_SENTINEL:\n\u001b[1;32m    187\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m default\n\u001b[0;32m--> 189\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(glob)\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(results) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdpath.get() globs must match only one leaf: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mglob\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'path_test'"
     ]
    }
   ],
   "source": [
    "print(__package__)\n",
    "args = argparser()\n",
    "'''---------------------------------------------------------'''\n",
    "std = configManager(args)\n",
    "\n",
    "device = std.parserinfo('device')\n",
    "modeflag = std.parserinfo('modeflag')\n",
    "window_grid = std.parserinfo('window_grid')\n",
    "channels = std.parserinfo('channels')\n",
    "option = std.parserinfo('Option')\n",
    "beta = std.parserinfo('beta')\n",
    "#lossmetric = std.parserinfo('lossfunctmetric')\n",
    "\n",
    "path = std.parserinfo('path_orig')\n",
    "path_aligned = std.parserinfo('path_aligned_orig')\n",
    "path_test = std.parserinfo('path_test')\n",
    "path_preexist_model = std.parserinfo('path_preexist_linear')\n",
    "path_preexist_modelGP = std.parserinfo('path_preexist_gp')\n",
    "\n",
    "#self.config, self.constrain, self.tasks, self.interpolation_type, self.option\n",
    "gp_params = std.get_config_vals(['noise_constraint','Task','interpolation_type','Option','Lengthscale','Initialization'])\n",
    "\n",
    "alphabets = ['?','-', 'A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y'] \n",
    "logdir = 'pretrained_deepseq'; outmodel_name = 'trained_model_BLAT_22_preprint_22feb2024.pth'\n",
    "\n",
    "\n",
    "print('Ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e911ad",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 396,
     "status": "ok",
     "timestamp": 1708322046199,
     "user": {
      "displayName": "variational alignment",
      "userId": "06771419765440515286"
     },
     "user_tz": -60
    },
    "id": "O3Gy6vGf-z7q",
    "outputId": "d6362516-ce61-47da-8bb2-afd4e2d6d4c9",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from src.gp_cpab.src.extra import experiment_utilities\n",
    "from src.dataLoaderDiffeo import datasetLoader\n",
    "\n",
    "# Raw Sequences, to see if we can align the sequences somehow\n",
    "\n",
    "c2i, i2c, i2i = seqsReader._predefine_encoding(alphabets)\n",
    "dataset_msa = seqsDatasetLoader(pathBLAT_data = path, alphabet = alphabets, enable_variable_length=True, device=device)\n",
    "#x1 = dataset_msa.prot_space\n",
    "dataset_aligned_msa = seqsDatasetLoader(pathBLAT_data = path_aligned, alphabet = alphabets, enable_variable_length=True, device=device)\n",
    "dataset_test = seqsDatasetLoader(pathBLAT_data = path_test, alphabet = alphabets, enable_variable_length=True, device=device)\n",
    "\n",
    "\n",
    "'''--------------------------------------------------------------------------------------------------------------------------'''\n",
    "'''dataset_msa.prot_space = replace_target_token_gaps(dataset_msa.prot_space, c2i, '-')[:,:,1:]\n",
    "dataset_aligned_msa.prot_space = replace_target_token_gaps(dataset_aligned_msa.prot_space, c2i, '-')[:,:,1:]\n",
    "alphabets=['L','Q','R']\n",
    "c2i, i2c, i2i = seqsReader._predefine_encoding(alphabets)'''\n",
    "'''--------------------------------------------------------------------------------------------------------------------------'''\n",
    "\n",
    "# Load data\n",
    "print('Loading data')\n",
    "\n",
    "batches = args.batch_size #16 # 448\n",
    "\n",
    "# initially the dimension is [448, 34, 21]. However as it is necesary to ignore\n",
    "# the batch size, I just create a tuple, by taking just the last 2 components from the size\n",
    "trainloader, testloader = Sequence_Data_Loader(dataset_msa, dataset_test=None, batch_size=batches)\n",
    "trainloader_pretrainer, testloader_pretrainer = Sequence_Data_Loader(dataset_aligned_msa, dataset_test=None, batch_size=100)\n",
    "#trainloader2, testloader2 = Sequence_Data_Loader(dataset_msa2, dataset_test=None, batch_size=batches)\n",
    "\n",
    "\n",
    "seq_size = ( [*dataset_msa.prot_space.shape][1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab54676b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 538
    },
    "executionInfo": {
     "elapsed": 7307,
     "status": "ok",
     "timestamp": 1708322053505,
     "user": {
      "displayName": "variational alignment",
      "userId": "06771419765440515286"
     },
     "user_tz": -60
    },
    "id": "dx5fh_P--z7q",
    "outputId": "4c8b8333-ff8a-483e-b9c5-0fe9e029f40f",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loading BLAT dataset\n",
    "deepseq = DeepSequence(seq_size, args.latent_dim, alphabets, device = device)\n",
    "optimizer_deepseq = torch.optim.AdamW(deepseq.parameters(), lr=args.lr*0.01)\n",
    "loss_function = LossFunctionsAlternatives()\n",
    "pretrained_deepseq_path = logdir + \"/\" + outmodel_name\n",
    "check_parameter_historic = []\n",
    "\n",
    "casted_device = \"cuda\" if device==\"gpu\" or device==\"cuda\" else \"cpu\" if device=='cpu'else 'mps'\n",
    "\n",
    "if os.path.isfile(pretrained_deepseq_path):\n",
    "    print (\"Loading Deformation Model...\")\n",
    "    deepseq.load_state_dict( torch.load(pretrained_deepseq_path, map_location=casted_device) )\n",
    "    print (\"Loaded\")\n",
    "\n",
    "else:\n",
    "    deepseq.training_representation(trainloader_pretrainer, loss_function, optimizer_deepseq, 1000, 1, logdir=logdir, out_modelname=outmodel_name, beta=1 )\n",
    "\n",
    "\n",
    "deepseq(dataset_msa.prot_space)\n",
    "plot_logos_probs(deepseq(dataset_msa.prot_space)[0], alphabets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e647d155",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1708322053506,
     "user": {
      "displayName": "variational alignment",
      "userId": "06771419765440515286"
     },
     "user_tz": -60
    },
    "id": "mzUNeVfP-z7r",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import src.gp_cpab.src.extra.utilities as ut\n",
    "import matplotlib.gridspec as gridspec\n",
    "import logomaker\n",
    "\n",
    "\n",
    "def get_GPinterpolation(TT, theta, Y, Ug,  n_batch, grid_length_for_GP, seq_len):\n",
    "    grids_trans_out = (TT.transform_grid(TT.grid,theta)*(seq_len - 1)).squeeze(1)\n",
    "    # creating the likelihood for interpolation\n",
    "    batch_Multitask_model, batch_multitask_likelihood = TT.sets_MultioutputGP_per_batches(grids_trans_out, Y, n_batch)\n",
    "    # To get distribution over Posteriors\n",
    "    trans_data, _, \\\n",
    "            lower,upper  = TT.predict_operation(Ug, batch_Multitask_model, batch_multitask_likelihood  )\n",
    "    trans_data = torch.reshape(trans_data, (n_batch, grid_length_for_GP, Y.shape[2]))\n",
    "    lower = torch.reshape( lower, (n_batch, grid_length_for_GP, Y.shape[2]) )\n",
    "    upper = torch.reshape( upper, (n_batch, grid_length_for_GP, Y.shape[2]) )\n",
    "\n",
    "    return grids_trans_out, trans_data, lower, upper\n",
    "\n",
    "def plot_GP_components_per_seq(Ug, gT, Trdata, Lw, Up, nrows, ncols, alphabets, title, fig):\n",
    "\n",
    "    for j, axs in enumerate(fig.axes[(nrows-1)*ncols : nrows*ncols]):\n",
    "        axs.plot( Ug.flatten().detach().numpy(), Trdata[:,j].detach().numpy(), 'b' )\n",
    "        axs.fill_between(Ug.flatten().detach().numpy(), Lw[:,j].detach().numpy(),  Up[:,j].detach().numpy(), alpha=0.3)\n",
    "        axs.legend([ 'Mean'])\n",
    "        axs.set_xticks( Ug.flatten().detach().numpy(), minor= True )\n",
    "        axs.axhline(0, color='grey', linewidth=0.8)\n",
    "        axs.set_title(alphabets[j])\n",
    "\n",
    "    #fig.tight_layout()\n",
    "\n",
    "\n",
    "def create_gridSpec_graphs(ncols=4,nrows=2,figsize=(25,12)):\n",
    "    fig = plt.figure(figsize=figsize) #(constrained_layout=True)\n",
    "    gs = gridspec.GridSpec(ncols=ncols, nrows=nrows, figure=fig)\n",
    "    for i in range(0, nrows):\n",
    "        for j in range(0,ncols):\n",
    "            fig.add_subplot(gs[i, j])\n",
    "    return fig, gs\n",
    "\n",
    "def plot_GP_components_alignment_channel(Ug,grids_trans_out, trans_data,lower,upper, fig2):\n",
    "    for cont,(U,gT, Trdata,Lw,Up) in enumerate(list(zip(Ug,grids_trans_out, trans_data,lower,upper))):\n",
    "        title='sequence {0}'.format(cont+1)\n",
    "        plot_GP_components_per_seq(U, gT, Trdata, Lw, Up, cont+1, Trdata.shape[1], alphabets=alphabets, title=title, fig = fig2)\n",
    "\n",
    "def heatmap_from_tensor_tmp(data, alphabet, title):\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    figure, ax = plt.subplots(figsize=(8,10))\n",
    "    #ax = plt.axes()\n",
    "    px = pd.DataFrame(data, columns=alphabet)\n",
    "    ff=sns.heatmap(px, linewidth=1, linecolor='w', annot=data, ax = ax)\n",
    "    ax.set_title(title)\n",
    "    plt.show()\n",
    "\n",
    "def plot_msa_from_preds(ts_aligned, alphabet, figsize = (5,4), show_axis=True):\n",
    "    fig, gs = create_gridSpec_graphs(ncols=1,nrows=ts_aligned.shape[0], figsize=figsize) #; fig.suptitle('MSA', fontsize='large')\n",
    "\n",
    "    alphabets_logo = [ i if i!='-' else 'X' for i in alphabet]\n",
    "    for cont, x_aligned in enumerate(ts_aligned):\n",
    "        domain_info_df = df_construction_aas([ x_aligned.unsqueeze(0).detach().numpy() ], x_aligned.unsqueeze(0).shape, alphabets_logo)\n",
    "\n",
    "        if show_axis == False:\n",
    "            fig.axes[cont].get_xaxis().set_visible(False)\n",
    "            fig.axes[cont].get_yaxis().set_visible(False)\n",
    "\n",
    "        logomaker.Logo(domain_info_df,\n",
    "                          color_scheme = 'NajafabadiEtAl2017',\n",
    "                          ax = fig.axes[cont],\n",
    "                          figsize=figsize, show_spines=False)\n",
    "\n",
    "\n",
    "\n",
    "# recon_data_train = model(dataset_msa.prot_space, deepseq)\n",
    "def get_GP_behavior_from_model_alignment(data,model, DS, len_grid = 13 , alphabets=['-','L','Q','R']):\n",
    "    model.eval()\n",
    "    Y = data\n",
    "    recon_data_train = model(data, DS)\n",
    "\n",
    "    x1_trans = recon_data_train[5]\n",
    "\n",
    "    out_DS_before_dir_trans = DS(x1_trans)[0]\n",
    "\n",
    "    TT=model.stn.st_gp_cpab\n",
    "    TT.interpolation_type = 'GP'\n",
    "    grid_length_for_GP = len_grid\n",
    "\n",
    "\n",
    "    #get the transformation from Big model\n",
    "    _, theta = model.sample_only_trans(Y)\n",
    "    seq_len = Y.shape[1]; n_batch = Y.shape[0]\n",
    "\n",
    "    ''' info about initial deformation '''\n",
    "    '''----------------------------------------------------------------------------------------------------------------------------------------------------------------------'''\n",
    "    initialTT_raw = TT.grid\n",
    "    initialTT_inv = TT.transform_grid(TT.grid,-theta)\n",
    "    initialTT_dir = TT.transform_grid(TT.grid,theta)\n",
    "    forward = torch.stack([initialTT_raw.flatten() ,initialTT_inv.flatten(), (initialTT_inv*(x1_trans.shape[1]-1)).flatten() ]).T\n",
    "    backward = torch.stack([initialTT_raw.flatten() ,initialTT_dir.flatten(), (initialTT_dir*(x1_trans.shape[1]-1)).flatten() ]).T\n",
    "    heatmap_from_tensor_tmp(forward.detach().numpy(), ['input','raw','scaled'], 'FORWARD SCHEME' )\n",
    "    heatmap_from_tensor_tmp(backward.detach().numpy(), ['input','raw','scaled'], 'BACKWARD SCHEME' )\n",
    "    '''----------------------------------------------------------------------------------------------------------------------------------------------------------------------'''\n",
    "\n",
    "    # grid creations and deformations based on theta estimated from big model\n",
    "\n",
    "    Ug=TT.uniform_meshgrid((grid_length_for_GP,grid_length_for_GP)).repeat(n_batch,1,1)*(seq_len - 1)\n",
    "\n",
    "    grids_trans_out, trans_data, lower, upper = get_GPinterpolation(TT, -theta, Y, Ug, n_batch, grid_length_for_GP, seq_len)\n",
    "    grids_dir_trans_out, dir_trans_data, lower_dir, upper_dir = get_GPinterpolation(TT, theta, out_DS_before_dir_trans, Ug, n_batch, grid_length_for_GP, seq_len)\n",
    "\n",
    "    ncols= grids_trans_out.shape[-1]; nrows = Ug.shape[0]\n",
    "    fig, gs = create_gridSpec_graphs(ncols=ncols,nrows=nrows, figsize=(25,20)); fig.suptitle('Sequence Alignment by Transformation', fontsize='large')\n",
    "    figdir, gsdir = create_gridSpec_graphs(ncols=ncols,nrows=nrows,figsize=(25,20)); figdir.suptitle('Sequence Reconstruction', fontsize='large')\n",
    "\n",
    "    plot_GP_components_alignment_channel(Ug,grids_trans_out, trans_data,lower,upper, fig)\n",
    "    plot_GP_components_alignment_channel(Ug,grids_dir_trans_out, dir_trans_data,lower_dir,upper_dir, figdir)\n",
    "\n",
    "\n",
    "\n",
    "#x = deepseq(trainloader_pretrainer.dataset.prot_space)\n",
    "#plot_msa_from_preds(x[0], alphabets, figsize=(15,10), show_axis=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409e41fb",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1708322053506,
     "user": {
      "displayName": "variational alignment",
      "userId": "06771419765440515286"
     },
     "user_tz": -60
    },
    "id": "GH9tPn7t-z7r",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def update_behavior_loss_batches(batch_loss_dict, batch_key, batch_historical):\n",
    "    if batch_key not in batch_loss_dict:\n",
    "        batch_loss_dict[batch_key] =  [batch_historical]\n",
    "    else:\n",
    "        batch_loss_dict[batch_key].append(batch_historical)\n",
    "\n",
    "def batch_plots(batch_loss_dict):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    if len(batch_loss_dict.keys())==1:\n",
    "         values= batch_loss_dict[ list(batch_loss_dict.keys())[0] ]\n",
    "         epochs_i = list(range(0,len(values)))\n",
    "         plt.plot(epochs_i, values)\n",
    "    else:\n",
    "        fig, ax = plt.subplots(len(batch_loss_dict.keys()),1, figsize=(4,10))\n",
    "        fig.tight_layout()\n",
    "\n",
    "        for i in batch_loss_dict.keys():\n",
    "            y = batch_loss_dict[i]\n",
    "            x = list(range(0,len(y)))\n",
    "            ax[i].set_title('Batch ' + str(i))\n",
    "            ax[i].plot(x,y)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def fit_VITAE_with_pretrained_DeepSeq(trainloader, optimizer, model, DS, n_epochs=10, warmup=1, logdir='',\n",
    "            testloader=None, eq_samples=1, iw_samples=1, beta=1.0, eval_epoch=10000, **kargs):\n",
    "        \"\"\" Fits the supplied model to a training set\n",
    "        Arguments:\n",
    "            trainloader: dataloader (of type torch.utils.data.DataLoader) that\n",
    "                contains the training data\n",
    "            n_epochs: integer, number of epochs to run\n",
    "            warmup: integer, the KL terms are weighted by epoch/warmup, so this\n",
    "                number determines the number of epochs before the KL-terms are\n",
    "                fully activated in the loss function\n",
    "            logdir: str, where to store the results\n",
    "            testloader: dataloader (of type torch.utils.data.DataLoader) that\n",
    "                contains the test data\n",
    "            eq_samples: integer, number of equality samples which the expectation\n",
    "                is calculated over\n",
    "            iw_samples: integer, number of samples the mean-log is calculated over\n",
    "            eval_epoch: how many epochs that should pass between calculating the\n",
    "                L5000 loglikelihood (very expensive to do)\n",
    "        \"\"\"\n",
    "\n",
    "        # Assert that input is okay\n",
    "        assert isinstance(trainloader, torch.utils.data.DataLoader), '''Trainloader\n",
    "            should be an instance of torch.utils.data.DataLoader '''\n",
    "        assert warmup <= n_epochs, ''' Warmup period need to be smaller than the\n",
    "            number of epochs '''\n",
    "\n",
    "        # Print stats\n",
    "        print('Number of training points: ', len(trainloader.dataset.prot_space))\n",
    "        if testloader: print('Number of test points:     ', len(testloader.dataset))\n",
    "\n",
    "        loss_function = LossFunctionsAlternatives()\n",
    "\n",
    "        #scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=550, gamma=0.1)\n",
    "        #import ipdb; ipdb.set_trace()\n",
    "\n",
    "        # Main loop\n",
    "        start = time.time()\n",
    "        for epoch in range(1, n_epochs+1):\n",
    "            progress_bar = tqdm(desc='Epoch ' + str(epoch) + '/' + str(n_epochs),\n",
    "                                total=len(trainloader.dataset), unit='samples')\n",
    "            train_loss = 0\n",
    "            # Training loop\n",
    "            #self.model.train()\n",
    "            # Decay Learning Rate\n",
    "            #scheduler.step()\n",
    "\n",
    "            if epoch == 205:\n",
    "                 print('From Here')\n",
    "            for i, data in enumerate(trainloader):\n",
    "                # Zero gradient\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Feed forward data\n",
    "                data = data.to(torch.float32)#.to(device)\n",
    "\n",
    "                switch = 1.0 if epoch > warmup else 0.0\n",
    "                out = model(data, DS, eq_samples, iw_samples, switch)\n",
    "\n",
    "                # Calculat loss\n",
    "                loss = loss_function(method = 'CE', input = out[0], target = data, forw_per=(0,2,1)) - beta*out[7]\n",
    "                #loss = loss_function(method = 'JSD', input = out[0], target = data, forw_per=(0,2,1)) # - out[7]\n",
    "\n",
    "\n",
    "                # Backpropegate and optimize\n",
    "                loss.backward()\n",
    "                #torch.nn.utils.clip_grad_value_(model.parameters(), 0.5)\n",
    "\n",
    "                ### UPDATE MODEL PARAMETERS\n",
    "                optimizer.step()\n",
    "\n",
    "\n",
    "                #########################################################\n",
    "                #########################################################\n",
    "                ### GRADIENT CLIPPING\n",
    "                #torch.nn.utils.clip_grad_value_(model.parameters(), 1.)\n",
    "                #torch.nn.utils.clip_grad_norm_(model.parameters(), 1., norm_type=2)\n",
    "                #########################################################\n",
    "                #########################################################\n",
    "\n",
    "                # Write to consoeeeddddddddwwww######hhhdffffffffffsssswqqq#l#\n",
    "                progress_bar.update(data.size(0))\n",
    "                progress_bar.set_postfix({'loss': str(loss.item())  })\n",
    "                #progress_bar.set_postfix({'loss': str(loss.item()) + ', lr rate: ' + str(scheduler.get_lr()[-1]) })\n",
    "                update_behavior_loss_batches(batch_loss_dict, i, loss.item())\n",
    "\n",
    "                # Save to tensorboard\n",
    "                iteration = epoch*len(trainloader) + i\n",
    "\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            progress_bar.close()\n",
    "\n",
    "        print('Total train time', time.time() - start)\n",
    "        #import pdb;pdb.set_trace()\n",
    "        # Save the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e24f57",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1117324,
     "status": "ok",
     "timestamp": 1708323170826,
     "user": {
      "displayName": "variational alignment",
      "userId": "06771419765440515286"
     },
     "user_tz": -60
    },
    "id": "d1Se9Ovj-z7s",
    "outputId": "ecce692e-dc40-4475-e40a-496aa91bfdc4",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "batch_loss_dict = {}\n",
    "logdir_density = 'pretrained_densities'; outmodel_name_density = 'trained_density_BLAT_22_preprint_22feb2024.pth'\n",
    "pretrained_density = logdir_density + \"/\" + outmodel_name_density\n",
    "#import ipdb; ipdb.set_trace()\n",
    "\n",
    "# Construct model\n",
    "# I do not add the pretrained deepsequence module to avoid\n",
    "# the backpropagation of such module during the optimization\n",
    "model = PGM_LA_latent_alignment(\n",
    "                    input_shape = seq_size, #img_size,\n",
    "                    config = std,\n",
    "                    latent_dim = args.latent_dim,\n",
    "                    encoder = get_encoder( args.ed_type.split(\",\")[0] ),\n",
    "                    decoder = get_decoder( args.ed_type.split(\",\")[1] ),\n",
    "                    outputdensity = args.density,\n",
    "                    ST_type = args.stn_type,\n",
    "                    alphabet_size = len(c2i),\n",
    "                    trans_parameters = ( [window_grid], device, gp_params ),\n",
    "                    diagonal_att_regions = [-575,575])#[-175,175] [-15,15] ) #[15,15] #if we increase it, major accuracy\n",
    "\n",
    "#model.get_deepsequence_module(deepseq)\n",
    "\n",
    "# Optimizer\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=1e-4)#, weight_decay=1e-19)#lr=1e-3 also works but we have to test with seed samples\n",
    "model_name = '/trained_model_softmax.pt'\n",
    "\n",
    "\n",
    "\n",
    "if os.path.isfile(pretrained_density):\n",
    "    print (\"Loading Density Model...\")\n",
    "    model.load_state_dict( torch.load(pretrained_density, map_location=casted_device) )\n",
    "    print (\"Loaded\")\n",
    "else:\n",
    "    fit_VITAE_with_pretrained_DeepSeq(trainloader, opt, model, deepseq, n_epochs=400, warmup=1, logdir='',\n",
    "            testloader=None, eq_samples=1, iw_samples=1, beta=1.0, eval_epoch=10) # initi\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1f28da",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1150,
     "status": "ok",
     "timestamp": 1708323171956,
     "user": {
      "displayName": "variational alignment",
      "userId": "06771419765440515286"
     },
     "user_tz": -60
    },
    "id": "KX_c51MY-z7s",
    "outputId": "8b297587-1137-44c5-d9bd-34b659761585",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_plots(batch_loss_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8476312",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 245718,
     "status": "ok",
     "timestamp": 1708323417669,
     "user": {
      "displayName": "variational alignment",
      "userId": "06771419765440515286"
     },
     "user_tz": -60
    },
    "id": "Ld9earng-z7t",
    "outputId": "e1193305-c6af-4286-e17d-217205b61d02",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "recon_data_train = model(dataset_msa.prot_space, deepseq)\n",
    "\n",
    "test_set_alignment = model(dataset_test.prot_space, deepseq)\n",
    "\n",
    "\n",
    "\n",
    "print('Done')\n",
    "\n",
    "from pyfiglet import Figlet\n",
    "f = Figlet(font='puffy' ) #slant\n",
    "\n",
    "print(recon_data_train[5])\n",
    "alignment = recon_data_train[5]\n",
    "\n",
    "\n",
    "\n",
    "plot_msa_from_preds(alignment.detach().cpu(), alphabets, figsize=(8,17), show_axis=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "print(f.renderText('INPUT'))\n",
    "plot_logos_probs(dataset_msa.prot_space.detach().cpu(), alphabets)\n",
    "\n",
    "print(f.renderText('RECONSTRUCTION'))\n",
    "plot_logos_probs(recon_data_train[0].detach().cpu(), alphabets)\n",
    "\n",
    "print(f.renderText('ALIGNMENT'))\n",
    "plot_logos_probs(recon_data_train[5].detach().cpu(), alphabets)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667a780c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 68680,
     "status": "ok",
     "timestamp": 1708323692755,
     "user": {
      "displayName": "variational alignment",
      "userId": "06771419765440515286"
     },
     "user_tz": -60
    },
    "id": "8fuJVBzs4yyC",
    "outputId": "e4f0805a-f9ce-4c41-efaf-3f94c06ae961",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_msa_from_preds(test_set_alignment[5].detach().cpu(), alphabets, figsize=(8,10), show_axis=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33daae22",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 35879,
     "status": "ok",
     "timestamp": 1708323516234,
     "user": {
      "displayName": "variational alignment",
      "userId": "06771419765440515286"
     },
     "user_tz": -60
    },
    "id": "Yw7ohKvZ-z7t",
    "outputId": "c13d5052-e142-4be1-ee84-4edcb940055f",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_msa_from_preds(trainloader.dataset.prot_space.detach().cpu(), alphabets, figsize=(8,17), show_axis=False)\n",
    "plot_msa_from_preds(dataset_test.prot_space.detach().cpu(), alphabets, figsize=(7,9), show_axis=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57c3258",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 847
    },
    "executionInfo": {
     "elapsed": 9389,
     "status": "error",
     "timestamp": 1708323525616,
     "user": {
      "displayName": "variational alignment",
      "userId": "06771419765440515286"
     },
     "user_tz": -60
    },
    "id": "MMZwIRmJ-z7t",
    "outputId": "22b708bc-50c8-45f0-dd41-28c3d38ac131",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "def MonteCarlo_sampleDS(DS, iters):\n",
    "    set_of_samples = [ DS.sample(1)[0] for i in range(0,iters)]\n",
    "    MC_sample = torch.mean(torch.stack(set_of_samples), dim=0)\n",
    "    return MC_sample\n",
    "\n",
    "def energy_info_DSsampling_vs_Raw(DS, batch_seqs):\n",
    "\n",
    "    #import ipdb; ipdb.set_trace()\n",
    "    batch_size = batch_seqs.shape\n",
    "    raw_seqs = batch_seqs; DS.eval()\n",
    "    sampled_seqs = MonteCarlo_sampleDS(DS,100)\n",
    "\n",
    "    sampled_seqs = sampled_seqs.permute(0,2,1)\n",
    "\n",
    "    # One Sampled Protein from Deep Sequence vs all 10 proteins\n",
    "    fig1vs10, axes1vs10 = plt.subplots(2, 5, figsize=(35,15))\n",
    "    fig1vs10.suptitle('Visualization of each protein in the set vs the same sampled protein from DeepSequence')\n",
    "    list_of_attentions = []\n",
    "    for cont, prot in enumerate(raw_seqs):\n",
    "        # One Sampled Protein from Deep Sequence vs all 10 proteins\n",
    "        ii,jj = ((1,cont-5), (0,cont))[cont < 5]\n",
    "        seq = torch.matmul(prot,sampled_seqs[0])\n",
    "        list_of_attentions.append(seq.detach().cpu())\n",
    "        axes1vs10[ii,jj].set_title('Protein ' + str(cont))\n",
    "        sns.heatmap(seq.detach().cpu().numpy(), linewidth=0.5, cmap='viridis', ax=axes1vs10[ii,jj])\n",
    "\n",
    "    plt.show()\n",
    "    return list_of_attentions\n",
    "\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def get_diagonal_attention(Matrix, comp, min_r, max_r, list_attention = []):\n",
    "    # Works assuming that we are dealing with square matrixes\n",
    "    if comp <= max_r and comp >= min_r:\n",
    "        list_attention.append( torch.cat( (torch.tensor([0.0]*abs(comp)) , torch.diagonal(Matrix, comp)) ) )\n",
    "        get_diagonal_attention(Matrix, comp-1, min_r, max_r, list_attention)\n",
    "\n",
    "def get_batch_diagonal_attention(Matrix, comp, min_r, max_r):\n",
    "    list_batch = []\n",
    "    list_attention = []\n",
    "    for m in Matrix:\n",
    "        get_diagonal_attention(m, comp, min_r, max_r, list_attention )\n",
    "        list_batch.append( torch.stack(list_attention) )\n",
    "        list_attention.clear()\n",
    "\n",
    "    batch_diag_attention = torch.stack(list_batch)\n",
    "    return batch_diag_attention\n",
    "\n",
    "\n",
    "attentions = energy_info_DSsampling_vs_Raw( deepseq, dataset_msa.prot_space.detach())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28b50a2",
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "aborted",
     "timestamp": 1708323525617,
     "user": {
      "displayName": "variational alignment",
      "userId": "06771419765440515286"
     },
     "user_tz": -60
    },
    "id": "5PPa0OPb-z7t",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "att = get_batch_diagonal_attention(attentions,10,-10, 10)\n",
    "\n",
    "figatt, axesatt = plt.subplots(2, 5, figsize=(35,15))\n",
    "figatt.suptitle('Visualization attention diagonals')\n",
    "\n",
    "for cont, at in enumerate(att):\n",
    "    # One Sampled Protein from Deep Sequence vs all 10 proteins\n",
    "    ii,jj = ((1,cont-5), (0,cont))[cont < 5]\n",
    "    axesatt[ii,jj].set_title('Protein ' + str(cont))\n",
    "    sns.heatmap(at.numpy(), linewidth=0.5, cmap='viridis', ax=axesatt[ii,jj])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "'''\n",
    "Matrix = torch.tensor([[1,2,3,4,5],[33,44,55,66,77],[333,444,555,666,777],[3333,4444,5555,6666,7777],[33333,44444,55555,66666,77777]])\n",
    "list_attention = []\n",
    "get_diagonal_attention(Matrix, 2, -2, 2, list_attention)\n",
    "\n",
    "Matrix2 = Matrix.repeat(2,1,1)\n",
    "get_batch_diagonal_attention(Matrix2, 2, -2, 2)\n",
    "\n",
    "print(list_attention)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8825f2eb",
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "aborted",
     "timestamp": 1708323525617,
     "user": {
      "displayName": "variational alignment",
      "userId": "06771419765440515286"
     },
     "user_tz": -60
    },
    "id": "nDQst9B6-z7u",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt\n",
    "from matplotlib.colors import LogNorm, Normalize\n",
    "\n",
    "fig1vs10, axes1vs10 = plt.subplots(2, 5, figsize=(35,15))\n",
    "fig1vs10.suptitle('Visualization of each protein in the set vs the same sampled protein from DeepSequence')\n",
    "\n",
    "def lognormalize(x):\n",
    "    a = np.logaddexp.reduce(x)\n",
    "    return np.exp(x - a)\n",
    "\n",
    "for cont, prot in enumerate(attentions):\n",
    "    # One Sampled Protein from Deep Sequence vs all 10 proteins\n",
    "    ii,jj = ((1,cont-5), (0,cont))[cont < 5]\n",
    "    axes1vs10[ii,jj].set_title('Protein ' + str(cont))\n",
    "    sns.heatmap(prot.numpy(), linewidth=0.5, cmap='viridis', ax=axes1vs10[ii,jj])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig1vs10_log, axes1vs10_log = plt.subplots(2, 5, figsize=(35,15))\n",
    "fig1vs10_log.suptitle('Visualization of each protein in the set vs the same sampled protein from DeepSequence')\n",
    "\n",
    "for cont, prot in enumerate(attentions):\n",
    "    # One Sampled Protein from Deep Sequence vs all 10 proteins\n",
    "    ii,jj = ((1,cont-5), (0,cont))[cont < 5]\n",
    "    axes1vs10_log[ii,jj].set_title('Protein ' + str(cont))\n",
    "    sns.heatmap( prot.numpy() , linewidth=0.5, cmap='viridis', ax=axes1vs10_log[ii,jj], norm=LogNorm())\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 11.114264,
   "end_time": "2024-02-22T13:16:50.270512",
   "environment_variables": {},
   "exception": true,
   "input_path": "5_BLAT_test_mutation_effects_pred_preprint.ipynb",
   "output_path": "res_5_BLAT_test_mutation_effects_pred_preprint.ipynb",
   "parameters": {},
   "start_time": "2024-02-22T13:16:39.156248",
   "version": "2.4.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}